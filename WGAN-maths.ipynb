{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf4b162-273d-445c-9d19-dbf87dfc6a7f",
   "metadata": {},
   "source": [
    "# How WGAN was invented "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e2c41-d4f0-4c9f-a93d-fa8b5054dffb",
   "metadata": {},
   "source": [
    "- Before we move ahead , we need to know some results from Lagrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71473db6-694e-457c-9a88-f30e09d7636a",
   "metadata": {},
   "source": [
    "### Lagrangian Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21fdb2-045f-4d6f-9259-9e92ea87f19a",
   "metadata": {},
   "source": [
    "- Optimzation problem (primal form or standard form) <br>\n",
    "$$\\underset{x}{\\min} f_0(x)$$\n",
    "subject to $$f_i(x) \\leq 0, i=1,2,...,m$$\n",
    "$$h_i(x) = 0, i=1,2,...,p$$\n",
    "`NOTE:` we have $m$ inequality constraints and $p$ equality constraints\n",
    "\n",
    "- Lagrange Idea: Augment the objective $f_0(x)$ and augment it with weighted sum as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8839d-0037-416a-9051-f27a1c728bd1",
   "metadata": {},
   "source": [
    "Define Lagrangian $L: \\mathbb R^n \\times \\mathbb R^m \\times \\mathbb R^p \\rightarrow \\mathbb R$ as <br>\n",
    "$$L(x, \\lambda, \\nu) = f_0(x) + \\sum_{i=1}^{m} \\lambda_i f_i(x) + \\sum_{i=1}^{p} \\lambda_i h_i(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961899f0-5f1d-468e-86ca-34bd2d736f58",
   "metadata": {},
   "source": [
    "- Here $\\lambda_i$ and $\\nu_i$ are lagrange multipliers and $\\lambda$ and $\\nu$ are also called dual variables\n",
    "- We can minimize the primal problem above with constraints, let denote solution to primal problem as $p^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d299b9a-97da-4b53-9b9d-11bbd533772c",
   "metadata": {},
   "source": [
    "###  **Lagrange dual function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d980d58-1782-4a37-91b1-ba4b30189e43",
   "metadata": {},
   "source": [
    "Lagrange Dual Function is function of auxiliary variable only <br>\n",
    "Lagrange dual function $g: \\mathbb R^m \\times \\mathbb R^p \\rightarrow \\mathbb R$\n",
    "$$g(\\lambda, \\nu) = \\inf_{x \\in \\mathbb D} L(x, \\lambda, \\nu) = \\inf_{x \\in \\mathbb D} \\big (f_0(x) + \\sum_{i=1}^{m} \\lambda_i f_i(x) + \\sum_{i=1}^{p} \\lambda_i h_i(x) \\big )$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59cd5d68-f828-43dc-be1b-65df26ea02de",
   "metadata": {},
   "source": [
    "- If the $\\inf_{x \\in D}$ is unbounded below, then dual is $-\\infty$\n",
    "- The dual is just a function of $\\lambda$, $\\nu$\n",
    "- `Observe`: Dual is affine in $\\lambda$ and $\\nu$, and $\\inf_{x \\in D} L(x, \\lambda, \\nu)$ is concave (i.e., Dual is concave)\n",
    "- Further $\\lambda_i \\geq 0$ since they are associated with inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c918e70a-674c-4195-b8ee-0abd667e0372",
   "metadata": {},
   "source": [
    "**Fact** If we minimize in primal, we need to maximize in dual\n",
    "- Let $d^*$ be the solution to $\\underset{\\lambda \\geq 0, \\nu}{\\sup} g(\\lambda, \\nu)$\n",
    "- **Fact**: $d^* \\leq p^*$ and gap to optimality is $|p^* - d^*|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f15d0f-7423-4669-9176-153d83f3dced",
   "metadata": {},
   "source": [
    "### Sufficient condition for $d^* = p^*$ (i.e., primal solution is same is dual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf6d77-606d-4806-a7b3-ddc807f3a42d",
   "metadata": {},
   "source": [
    "- $f_0$ is convex\n",
    "- **Slater conditions**: non-linear equations in inequality constraints are strictly satisfied for some feasible x. Notice, we have $m$ inequality constraints, out of which atmost $m$ inequality constraints may be non-linear in $x$, so those non-linear inequality constraints are satisified with strict inequality for some feasible $x$, then we get strong duality\n",
    "- `NOTE:` both above conditions are only sufficient for strong duality but not neccessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81df4eb-951d-4237-aa6c-5a4f0b8cf0ca",
   "metadata": {},
   "source": [
    "### Interesting Fact : when can you swap $\\sup \\inf$ with $\\inf \\sup$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e22f2eed-6b32-4edb-b948-a016d5765d11",
   "metadata": {},
   "source": [
    "- We know that solution to dual is given as <br>\n",
    "$$\\underset{\\lambda \\geq 0, \\nu}{\\sup}g(\\lambda, \\nu) = \\underset{\\lambda \\geq 0, \\nu}{\\sup} \\inf_{x \\in \\mathbb D} L(x, \\lambda, \\nu) = \\underset{\\lambda \\geq 0, \\nu}{\\sup} \\inf_{x \\in \\mathbb D} \\big (f_0(x) + \\sum_{i=1}^{m} \\lambda_i f_i(x) + \\sum_{i=1}^{p} \\nu_i h_i(x) \\big )$$\n",
    "- `Result:` If strong duality holds, then you can swap $\\underset{\\lambda \\geq 0, \\nu}{\\sup} \\inf_{x \\in \\mathbb D}$ with $\\inf_{x \\in \\mathbb D} \\underset{\\lambda \\geq 0, \\nu}{\\sup}$\n",
    "- **Proof**:\n",
    "- By definition, the solution to dual problem $\\underset{\\lambda \\geq 0, \\nu}{\\sup}g(\\lambda, \\nu) = d^*$ which is equal to $p^*$ since we assume strong duality, therefore we have <br>\n",
    "$$\\underset{\\lambda \\geq 0, \\nu}{\\sup}g(\\lambda, \\nu) = d^* = p^* = \\underset{\\lambda \\geq 0, \\nu}{\\sup} \\inf_{x \\in \\mathbb D} \\big (f_0(x) + \\sum_{i=1}^{m} \\lambda_i f_i(x) + \\sum_{i=1}^{p} \\nu_i h_i(x) \\big ) $$\n",
    "- Now if we swap $\\underset{\\lambda \\geq 0, \\nu}{\\sup} \\inf_{x \\in \\mathbb D}$ with $\\inf_{x \\in \\mathbb D} \\underset{\\lambda \\geq 0, \\nu}{\\sup} $ in the extreme RHS in the above equation, and we get\n",
    "$$\\inf_{x \\in \\mathbb D} \\underbrace{\\underset{\\lambda \\geq 0, \\nu}{\\sup} \\big (f_0(x) + \\sum_{i=1}^{m} \\lambda_i f_i(x) + \\sum_{i=1}^{p} \\nu_i h_i(x) \\big ) }_{A}$$\n",
    "- Now, if we show $A = \\underset{\\lambda \\geq 0, \\nu}{\\sup} \\big (f_0(x) + \\sum_{i=1}^{m} \\lambda_i f_i(x) + \\sum_{i=1}^{p} \\nu_i h_i(x) \\big ) = f_0(x)$ with $f_i \\leq 0$, and $h_i = 0$, then the above equation becomes <br>\n",
    "$$p^* = \\inf_{x \\in \\mathbb D} \\underbrace{\\underset{\\lambda \\geq 0, \\nu}{\\sup} \\big (f_0(x) + \\sum_{i=1}^{m} \\lambda_i f_i(x) + \\sum_{i=1}^{p} \\nu_i h_i(x) \\big ) }_{A} = \\inf_{x \\in \\mathbb D} f_0(x), f_i \\leq 0, h_i = 0$$\n",
    "- In order to prove this, just take deriavtive of $A$ w.r.t to $\\lambda_i$ and $\\nu_i$ and equate to 0 in order to find $\\sup$\n",
    "  - $\\frac{\\partial A}{\\lambda_i}= f_i(x) = 0$, $\\frac{\\partial A}{\\nu_i}= h_i(x) = 0$\n",
    "  - Also note that no need to check for second derivative in this case, since it is linear function in $\\lambda,\\nu$ and linear functions have max, min at border points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fef54-02c8-48d8-a7bd-0375a32b8f65",
   "metadata": {},
   "source": [
    "### Distance measure for WGAN: Wasserstein or Earth mover distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea0c844-0fc1-4b80-ae9d-6c38d647abed",
   "metadata": {},
   "source": [
    "**General Wasserstein distance is given by**\n",
    "$$W_r(p,q) = \\inf_{\\pi \\in \\Pi} \\big (\\int_{\\mathbb X \\times \\mathbb X} d(x,y)^r \\pi(x,y)\\partial x \\partial y\\big )^{\\frac{1}{r}} = \\inf_{\\pi \\in \\Pi} \\big (\\mathbb E_{(x,y) \\sim \\pi} d(x,y)^r \\big )^{\\frac{1}{r}}$$\n",
    "- $(x,y) \\sim \\pi$ and $\\Pi$ is set of all possible joint distributions on sample space $\\mathbb X \\times \\mathbb X$ with marginals $p$ and $q$\n",
    "- $d(x,y)$ is distance measure between $x,y$, like $||x-y||_2$\n",
    "- We choose $r=1$ and $d(x,y) = ||x-y||_2$, therefore $W_1(p,q)$ is given by <br>\n",
    "$$W_1(p,q) = \\inf_{\\pi \\in \\Pi} \\int_{\\mathbb X \\times \\mathbb X} ||x-y||_2 \\pi(x,y) \\partial x \\partial y = \\inf_{\\pi \\in \\Pi} \\mathbb E_{(x,y) \\sim \\pi} ||x-y||_2$$\n",
    "- Here, think of $p$ as real data distribution from where your samples are coming from and $q=p_\\theta$ is the parameterized distribution which you want to make it close to $p$\n",
    "- Working with above formulation is not easy for computing $W_1(p,q)$, so we have another theorem **KR theorem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d75bd-a91e-4a20-9080-f2d392dc4458",
   "metadata": {},
   "source": [
    "### Kantorovich - Rubinstein (KR) Theorem for $W_1(p,q)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53fd45-56c4-4ba7-97c4-082a312fda68",
   "metadata": {},
   "source": [
    "**KR theorem** <br>\n",
    "$$W_1(p,q) = \\inf_{\\pi \\in \\Pi} \\mathbb E_{(x,y) \\sim \\pi} ||x-y||_2 = \\underset{||h||_2 \\leq 1}{\\sup} \\big (\\mathbb E_{x \\sim p}h(x) - \\mathbb E_{y \\sim q}h(y) \\big )$$\n",
    "- $h$ is set of all functions which are 1-Lipschitz, which implies\n",
    "  - $|h(x)-h(y)| \\leq ||x-y||_2$, (in words, the absolute value of gradient is bounded by 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e84bba-e65f-4d4e-9060-a811287009a5",
   "metadata": {},
   "source": [
    "### **Proof of KR theorem**\n",
    "- Lets set up Lagrangian for $\\mathbb E_{(x,y) \\sim \\pi} ||x-y||_2$, where marginal of $\\pi$ are $p$ and $q$, these marginals will give us equality constraints for Lagrangian as shown below\n",
    "- $p(x) = \\int \\pi(x,y) \\partial x \\partial y$, $q(y) = \\int \\pi(x,y) \\partial x \\partial y$, which gives us equality constraints as follows:\n",
    "- $p(x) - \\int \\pi(x,y) \\partial x \\partial y = 0$, $q(y) - \\int \\pi(x,y) \\partial x \\partial y = 0$\n",
    "- `NOTE:`, here, we have no inequality constraints, further our objective function is a norm ($||x-y||_2$) which is convex, and $\\mathbb E$ operator preserves convexity, therefore strong duality holds in this case as Slater conditions are satisifed\n",
    "- Our Lagrangian is given by\n",
    "  $$L(\\pi, \\lambda, \\nu) = \\int_{\\mathbb X \\times \\mathbb X} ||x-y||_2 \\pi(x,y) \\partial x \\partial y + \\int f(x)\\big(p(x) - \\int \\pi(x,y)\\big) \\partial x \\partial y + \\int g(y)\\big(q(y) - \\int \\pi(x,y)\\big) \\partial x \\partial y$$\n",
    "  - Here, $f(x), g(y)$ are lagrange multipliers for continous case\n",
    "- Simplifying further gives,\n",
    "  $$L(\\pi, \\lambda, \\nu) = \\mathbb E_{x \\sim p}f(x) + \\mathbb E_{y \\sim q}g(y) + \\int  \\big( ||x-y||_2 - f(x) -g(y) \\big) \\pi(x,y) \\partial x \\partial y$$\n",
    "- where $p$ and $q$ are marginals of $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f8ab2b-9195-4401-b429-9e3731de0e63",
   "metadata": {},
   "source": [
    "- Our $W_1(p,q)$ is given by $\\underset{\\pi \\in \\Pi}{\\inf} L(\\pi, \\lambda, \\nu) = \\underset{\\pi \\in \\Pi}{\\inf} \\mathbb E_{x \\sim p}f(x) + \\mathbb E_{y \\sim q}g(y) + \\int  \\big( ||x-y||_2 - f(x) -g(y) \\big) \\partial x \\partial y$\n",
    "- Solving using duality, we get $\\underset{f,g}{\\sup} \\underset{\\pi \\in \\Pi}{\\inf} L(\\pi, \\lambda, \\nu) = \\underset{f,g}{\\sup} \\underset{\\pi \\in \\Pi}{\\inf} \\mathbb E_{x \\sim p}f(x) + \\mathbb E_{y \\sim q}g(y) + \\int  \\big( ||x-y||_2 - f(x) -g(y) \\big) \\pi(x,y) \\partial x \\partial y$\n",
    "- Simplifying further, we get  $\\underset{f,g}{\\sup} \\underset{\\pi \\in \\Pi}{\\inf} L(\\pi, \\lambda, \\nu) = \\underset{f,g}{\\sup} \\big(  \\mathbb E_{x \\sim p}f(x) + \\mathbb E_{y \\sim q}g(y) + \\underbrace{\\underset{\\pi \\in \\Pi}{\\inf} \\int  \\big( ||x-y||_2 - f(x) -g(y) \\big) \\pi(x,y) \\partial x \\partial y}_{B} \\big)$\n",
    "- If $(||x-y||_2 - f(x) -g(y)) \\leq 0$, then we can make $B \\rightarrow -\\infty$, it is just by putting entire probability mass at any particular $x_0, y_0$ for which $(||x_0-y_0||_2 - f(x_0) -g(y_0)) \\leq 0$, thereby making $B \\rightarrow -\\infty$. This is hand wavy arguement, we will leave it at there\n",
    "- Therefore we consider  $(||x-y||_2 - f(x) -g(y)) \\geq 0$, in this case we can choose $\\pi(x,y) = 0$ wherever $(||x-y||_2 - f(x) -g(y)) \\geq 0$ and $\\pi(x,y) \\neq 0$ elsewhere which makes $B = 0$\n",
    "- Finally we have $\\underset{f,g}{\\sup} \\underset{\\pi \\in \\Pi}{\\inf} L(\\pi, \\lambda, \\nu) = \\underset{f,g}{\\sup} \\big(  \\mathbb E_{x \\sim p}f(x) + \\mathbb E_{y \\sim q}g(y) \\big)$ with constraint as $(||x-y||_2 - f(x) -g(y)) \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730f88d-f139-4c6a-a292-6552d115d187",
   "metadata": {},
   "source": [
    "- Now by using Lagrange duality, we have reached at $W_1(p,q) = \\underset{f,g}{\\sup} \\big(  \\mathbb E_{x \\sim p}f(x) + \\mathbb E_{y \\sim q}g(y) \\big)$ where $(||x-y||_2 - f(x) -g(y)) \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8720d677-d471-4344-934a-f5f772cb7d74",
   "metadata": {},
   "source": [
    "`Remember:` We need to show that $W_1(p,q) = \\underbrace{\\underset{f,g}{\\sup} \\big(  \\mathbb E_{x \\sim p}f(x) + \\mathbb E_{y \\sim q}g(y) \\big)}_{C}$ where $\\underbrace{(||x-y||_2 - f(x) -g(y)) \\geq 0}_{C} = \\underbrace{\\underset{||h||_2 \\leq 1}{\\sup} \\big (\\mathbb E_{x \\sim p}h(x) - \\mathbb E_{y \\sim q}h(y) \\big )}_{D}$\n",
    "- We want to show $C=D$, for this we will\n",
    "  - Step1: show that $C \\geq D$\n",
    "  - Step2: show that $C \\leq D$\n",
    "  - Step1 and Step2 will imply: $C=D$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff8bee-3227-48fe-a399-f9461005bed1",
   "metadata": {},
   "source": [
    "- `Lets show step1:` $C \\geq D$, for that recall that $h$ in $D$ is 1-Lipschitz function which means $|h(x)-h(y)| \\leq ||x-y||_2$\n",
    "- See in 1-Lip equation, multiply both sides by $\\pi(x,y)$ and integrate which gives $\\int \\big( h(x)-h(y)\\big)\\pi(x,y) \\partial x \\partial y \\leq \\int \\big( ||x-y||_2 \\pi(x,y) \\big) \\partial x \\partial y $, simplifying LHS further gives\n",
    "- $\\mathbb E_{x \\sim p}h(x) - \\mathbb E_{y \\sim q} h(y) \\leq \\int \\big( ||x-y||_2 \\pi(x,y) \\big) \\partial x \\partial y$, $p, q$ are marginals of $\\pi(x,y)$\n",
    "- Now the RHS above is true for any $\\pi(x,y)$, therefore it must be true for $\\underset{\\pi}{\\inf}$ which gives $\\mathbb E_{x \\sim p}h(x) - \\mathbb E_{y \\sim q} h(y) \\leq \\underset{\\pi}{\\inf} \\int \\big( ||x-y||_2 \\pi(x,y) \\big) \\partial x \\partial y = W_1(p,q)$. Hence Step1 proved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230ebfb-6a56-4068-8fe7-c908c6ec0bda",
   "metadata": {},
   "source": [
    "- `Now lets show Step2`: Before we show step2, we need to learn one more thing **Infimal convolution**\n",
    "- **Infimal convolution**: $k(x) = \\underset{u}{\\inf} \\big( ||x-u||_2 - g(u) \\big)$\n",
    "  - $k(x)$ is convex in $x$ as it is norm function ($u$ disappers after taking $\\underset{u}{\\inf}$)\n",
    "  - $k(x)$ is 1-Lipschitz\n",
    "    - `Proof:` $k(x) = \\underset{u}{\\inf} \\big( ||x + y -y -u||_2 - g(u) \\big) \\leq  \\big( ||x + y -y -u||_2 - g(u) \\big)$ \n",
    "    - Apply traingle inequality to the RHS above, we get $k(x) \\leq  \\big( ||x -y|||_2 + ||y -u||_2 - g(u) \\big)$\n",
    "    - The RHS above is true for any $u$, so it must be true over $\\underset{u}{\\inf}$, therefore we get\n",
    "    - $k(x) \\leq  \\underset{u}{\\inf} \\big( ||x -y|||_2 + ||y -u||_2 - g(u) \\big) = ||x -y|||_2 + \\underbrace{\\underset{u}{\\inf} \\big(||y -u||_2 - g(u) \\big)}_{k(y)}$\n",
    "    - Therefore, from above we get $k(x) - k(y) \\leq ||x -y|||_2$\n",
    "    - Similarly, by symmetry we have $k(y) - k(x) \\leq ||y-x|||_2$\n",
    "    - Therefore, $|k(x) - k(y)| \\leq ||x -y|||_2$, hence $k(x)$ is 1-Lipschitz function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5847c-37d2-4ce4-b597-fb9930bfa3f5",
   "metadata": {},
   "source": [
    "- `Now continue with step2:` We need to show that $C \\leq D$, where, $$\\underbrace{\\underset{f,g}{\\sup} \\big(  \\mathbb E_{x \\sim p}f(x) + \\mathbb E_{y \\sim q}g(y) \\big)}_{C} and  \\underbrace{(||x-y||_2 - f(x) -g(y)) \\geq 0}_{C} = \\underbrace{\\underset{||h||_2 \\leq 1}{\\sup} \\big (\\mathbb E_{x \\sim p}h(x) - \\mathbb E_{y \\sim q}h(y) \\big )}_{D}$$\n",
    "- start with constraint above: $||x-y||_2 - f(x) -g(y)) \\geq 0$, rewriting as $f(x) + g(y) \\leq ||x-y||_2$\n",
    "- Since we want to show $C \\leq D$ and $D$ has 1-Lip function, so we will try expressing $f$ and $g$ in terms of 1-Lip function by using infimal convolution since it is 1-Lip\n",
    "- So, we have from above, $f(x) + g(y) \\leq ||x-y||_2$ which is same as $f(x) \\leq ||x-y||_2 - g(y)$, which is same as $f(x) \\leq \\underset{y}{\\inf}\\big(||x-y||_2 - g(y) \\big)$ which implies $f(x) \\leq k(x)$ where $k$ is infimal convolution of $g$ and 1-Lip, taking $\\mathbb E$ on both sides gives $\\mathbb E_{x \\sim p}f(x) \\leq E_{x \\sim p} k(x)$\n",
    "- Also, since $f(x) \\leq \\big(||x-y||_2 - g(y) \\big)$, put $x=y$, we get $f(x) \\leq -g(x)$. Since $k(x) = \\underset{y}{\\inf}\\big(||x-y||_2 - g(y) \\big)$, therefore $k(x) \\leq -g(x)$, change of variables, we have $k(y) \\leq -g(y)$, take $\\mathbb E_{y \\sim q}$ on both sides, we get $ \\mathbb E_{y \\sim q} k(y) \\leq -\\mathbb E_{y \\sim q} g(y)$\n",
    "- Now we have, $\\mathbb E_{x \\sim p}f(x) \\leq E_{x \\sim p} k(x)$ and $ \\mathbb E_{y \\sim q} k(y) \\leq -\\mathbb E_{y \\sim q} g(y)$, add both to get\n",
    "- $\\mathbb E_{x \\sim p}f(x) + \\mathbb E_{y \\sim q} g(y) \\leq E_{x \\sim p} k(x) - \\mathbb E_{y \\sim q} k(y)$, where $k(x)$ is infimal convolution of $g(y)$ and $k(y)$ is infimal convolution of $f(x)$ and $k$ is 1-Lip. Hence, step2 proved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69713f61-8b2d-4ccc-aedf-4a105c2063d5",
   "metadata": {},
   "source": [
    "- Now we have proved KR theorem, therefore we have\n",
    "$$W_1(p,q) = \\inf_{\\pi \\in \\Pi} \\mathbb E_{(x,y) \\sim \\pi} ||x-y||_2 = \\underset{||h||_2 \\leq 1}{\\sup} \\big (\\mathbb E_{x \\sim p}h(x) - \\mathbb E_{y \\sim q}h(y) \\big )$$\n",
    "  - $h$ is set of all functions which are 1-Lipschitz, which implies\n",
    "  - $|h(x)-h(y)| \\leq ||x-y||_2$, (in words, the absolute value of gradient is bounded by 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181b8015-8014-4bbb-8419-984d4db9c615",
   "metadata": {},
   "source": [
    "### Understanding KR final result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f10727f-ff53-4eab-a949-31f9e7286269",
   "metadata": {},
   "source": [
    "- We have $$W_1(p,q) = \\inf_{\\pi \\in \\Pi} \\mathbb E_{(x,y) \\sim \\pi} ||x-y||_2 = \\underset{||h||_2 \\leq 1}{\\sup} \\big (\\mathbb E_{x \\sim p}h(x) - \\mathbb E_{y \\sim q}h(y) \\big )$$\n",
    "- Imagine $p$ is true data distribution and $q$ is the parameterized distribution which you want to make closer to $p$\n",
    "- Usually, samples from $q$ comes from a neural network (with parameters $\\theta$) that maps some known distribution (like normal, uniform) to $q_\\theta$. So $y=g_\\theta(z)$. Therefore, $\\mathbb E_{y \\sim q}h(y)$ becomes $\\mathbb E_{z \\rightarrow g_\\theta(z)}h(g_\\theta(z))$\n",
    "- `Remember our final goal:` it is to minimize the $W_1(p,q)$, therefore we want to do following\n",
    "  $$\\underset{\\theta}{\\inf}\\underset{||h||_2 \\leq 1}{\\sup} \\big (\\mathbb E_{x \\sim p}h(x) - \\mathbb E_{z \\rightarrow g_\\theta(z)}h(g_\\theta(z)) \\big )$$\n",
    "    - Take your real samples and pass it through 1-Lip function $h$ to approximate $\\mathbb E_{x \\sim p}h(x)$\n",
    "    - Take samples from your parameterized distribution and pass it through same 1-Lip function $h$ to approximate $\\mathbb E_{z \\rightarrow g_\\theta(z)}h(g_\\theta(z))$\n",
    "    - **critic**: job is to maximize $\\big (\\mathbb E_{x \\sim p}h(x) - \\mathbb E_{z \\rightarrow g_\\theta(z)}h(g_\\theta(z)) \\big )$\n",
    "    - **adversary**: job is to minimize $\\big (\\mathbb E_{x \\sim p}h(x) - \\mathbb E_{z \\rightarrow g_\\theta(z)}h(g_\\theta(z)) \\big )$\n",
    "    - `NOTE:`in WGAN formulation using KR, we do not have any $\\log$ in the optimization as we do not have any $\\exp$ terms involved here unlike Goodfellow GAN formulation (see below), so issues related to $\\log$ in optimization in Vanilla GAN which makes training harder are not present in WGAN\n",
    "    - Usually, critic job is performed again through a neural network which behaves as $h$, we have to ensure that $h$ is 1-Lip (usually done by gradient clipping which is a bit unstable, another approach is gradient penalty term in the optimzation cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa0687-909f-4bde-a356-4bf8fe6c99a2",
   "metadata": {},
   "source": [
    "### Compare it with Goodfellow GAN result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc62ce7-b792-474a-9fda-11bdd14cb99e",
   "metadata": {},
   "source": [
    "$\\underset{\\theta}{\\min}D_{f}(p||p_\\theta) = \\underset{\\theta}{\\min} \\underset{\\phi}{\\sup} \\big(\\mathbb E_{x \\sim p(x)}\\log d_\\phi(x) + \\mathbb E_{z} \\log (1 - d_\\phi(g_\\theta(z))) \\big)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
